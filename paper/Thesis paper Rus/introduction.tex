\newpage


\section{Введение}
\paragraph{Актуальность темы.} 
    Уменьшение размерности пространства обучаемых параметров в задаче адаптации к домену упрощает процесс обучения и улучшает вычислительную эффективность. Путем сокращения количества параметров, которые необходимо обновить во время обучения, модель может потенциально быстрее сходиться и затрачивать меньше вычислительных ресурсов; это связано с структурой нейронной сети~--- ее сложность и потребление ресурсов зависят от количества обучаемых параметров. Уменьшение размерности может быть особенно важным в сценариях адаптации к домену, где обрабатываются большие объемы данных и происходит обучение с большим числом параметров.
    
    Методы, направленные на решение проблемы снижения размерности: метод главных компонент~\cite{wold1987principal} и его адаптации: тензорное разложение~\cite{oseledets2011tensor,qazi2024gpt}, каноническое полиадическое разложение~\cite{zare2018extension} выбирают наиболее важные векторы признаков из набора данных, используя сингулярное разложение матрицы для нахождения первых~$K$ собственных векторов с наибольшим собственным значением. Методы, осуществляющие отбор признаков: регуляризация LASSO (L1)~\cite{fonti2017feature}, оценка Фишера~\cite{gu2012generalized} или тест Хи-квадрат~\cite{zhai2018chi}. Метод снижения размерности, основанный на дообучении больших текстовых моделей~--- дистилляция~\cite{hsieh2023distilling}; в этом методе большая генеративная модель является \textit{учителем}, а меньшая~--- \textit{учеником}. Модель ученика обучают с использованием прогнозов \textit{учителя}. Эти идеи были впервые представлены в работах Дж. Хинтона~\cite{hinton2015nips} и В.Н. Вапника~\cite{vapnik2015learning}.

    Метод, рассмотренный в данной работе~--- низкоранговое разложение~(англ. Low Rank Adaptation)~\cite{hu2021lora}, который разработан на основе идеи о том, что предварительно обученные языковые модели имеют низкую внутреннюю размерность и могут эффективно обучаться, несмотря на проецирование на меньшее подпространство~\cite{aghajanyan2020intrinsic}. Данный метод, как и метод главных компонент, использует сингулярное разложение матрицы для нахождения низкоранговых приближений матрицы весов. LoRA используется для решения различных проблем seq2seq, таких как:~\cite{zhang2023adding,dettmers2024qlora,dai2024instructblip}. Этот подход особенно популярен в задачах преобразования видео в текст, так как этим задачам свойственны вариативность распределения входных данных и разнообразие задач, обусловленные дополнительным визуальным входным данным~\cite{dai2024instructblip}.

    В данной работе метод LoRA применяется к задаче обнаружения текстов, написанных искуственным интеллектом или человеком.
    Задача обнаружения текстов, написанных искусственным интеллектом стала особенно популярна с выходом новых больших моделей от OpenAI и Google~\cite{OpenAI, Google}, так как определить кем написан тот или иной текст все сложнее~\cite{anderson2023ai, weber2023testing}. В данной статье мы работали над дообучением популярной модели RoBerta с использованием LoRA адаптеров с целью понижения размерности пространства обучаемых параметров. Предполагается, что LoRA может быть так же эффективен в решении задач классификации, как и в задачах генерации: LoRA доказала свою эффективность во многих задачах генерации~\cite{dettmers2024qlora,hu2021lora,dai2024instructblip}.

Как отмечено в работах~\cite{he2023mgtbench,abdali2024decoding}, все подходы к решению задачи классификации ai текстов разделяются на два типа: ориентированные на анализ признакового пространства и ориентированные на дообучение моделей. 
 Анализ признакового пространства основывается на извлечении и анализе характеристик текста~--- лексических, синтаксических, семантических или стилистических характеристик:~\cite{liang2023gpt,yu2023gpt,yang2023dna}.
 Дообучение больших языковых моделей основывается на изучении параметров и возможностей модели и последующем дообучении модели к задаче классификации текстов:~\cite{wolf2019huggingface,wolf2019huggingface,qasim2022fine}.


\textbf{Цели работы.}
Исследуются методы снижения пространства обучаемых параметров при помощи сингулярного разложения матриц, а также кооректность применения изучаемых методов к задаче классификации текстов. 

\textbf{Методы исследования.}
Применяется низкоранговое разложение~(англ. Low Rank Adaptation) к матрицам параметров в больших языковых моделях. Используются статистические методы оценки функции минимизации функции потерь, а также свойтва матриц для доказательства применимости предложенного метода к задаче классификации.

\textbf{Теоретическая значимость.}
В работе проведен теоретический анализ проблемы снижения размерности пространства обучаемых параметров. Доказана теорема об применимости модели BERT~\cite{vaswani2017attention} с адаптером LoRA к задаче многоклассовой классификации. 

\textbf{Практическая значимость.}
Проведен вычислительный эксперимент, показывающий улучшение качества и экономию ресурсов при решении задачи классификации текстов.