\newpage


\section{Введение}
\paragraph{Актуальность} 
    Уменьшение размерности пространства обучаемых параметров в задаче адаптации к домену упрощает процесс обучения и улучшает вычислительную эффективность. Путем сокращения количества параметров, которые необходимо обновить во время обучения, модель может потенциально быстрее сходиться и затрачивать меньше вычислительных ресурсов; это связано с структурой нейронной сети - ее сложность и потребление ресурсов зависят от количества обучаемых параметров. Уменьшение размерности может быть особенно важным в сценариях адаптации области, где обрабатываются большие объемы данных и происходит обучение с большим числом параметров.
    
\paragraph{Анализ методов решения задачи понижения размерности пространства обучаемых параметров}
    Методы, направленные на решение проблемы снижения размерности: метод главных компонент~\cite{wold1987principal} и его адаптации: тензорное разложение~\cite{oseledets2011tensor},~\cite{qazi2024gpt}, каноническое полиадическое разложение~\cite{zare2018extension} выбирают наиболее важные векторы признаков из набора данных, используя сингулярное разложение матрицы для нахождения первых K собственных векторов с наибольшим собственным значением. Методы, осуществляющие отбор признаков: регуляризация LASSO (L1)~\cite{fonti2017feature}, оценка Фишера~\cite{gu2012generalized} или Тест Хи-квадрат~\cite{zhai2018chi}. Метод снижения размерности, основанный на дообучении больших текстовых моделей - дистилляция~\cite{hsieh2023distilling}; в этом методе большая генеративная модель является «учителем», а меньшая - «учеником». Модель «ученика» обучают с использованием прогнозов "учителя". Эти идеи были впервые представлены в работах Дж. Хинтона~\cite{hinton2015nips} и В.Н. Вапника~\cite{vapnik2015learning}.

    Метод, рассмотренный в данной работе - низкоранговое разложение(англ. Low Rank Adaptation)~\cite{hu2021lora}, который разработан на основе идеи о том, что предварительно обученные языковые модели имеют низкую "внутреннюю размерность" и могут эффективно обучаться, несмотря на проецирование на меньшее подпространство~\cite{aghajanyan2020intrinsic}. Данный метод, как и метод главных компонент, использует сингулярное разложение матрицы для нахождения низкоранговых приближений матрицы весов. 

    Здесь, метод LoRA применяется к одной из наиболее ресурсозатратных задач: обнаружение текстов, написанных искуственным интеллектом(или человеком).
    Проблема обнаружения текстов, написанных искусственным интеллектом, не теряет популярности с годами в научном сообществе. Особенно сейчас, когда отличить тексты, написанные человеком, от текстов, написанных искусственным интеллектом, является основной проблемой для агентств по борьбе с плагиатом. В данной статье мы работали над fine tuning'ом популярной LLM RoBerta с использованием LoRA адаптеров с целью понижения размерности пространства обучаемых параметров. Предполагается, что LoRA может быть так же эффективен в решении задач классификации, как и в задачах генерации: LoRA доказала свою эффективность во многих задачах генерации~\cite{dettmers2024qlora},~\cite{hu2021lora},~\cite{dai2024instructblip}.

\paragraph{Анализ методов решения задачи классификации ai текстов}
Как отмечено в работах~\cite{he2023mgtbench} и~\cite{abdali2024decoding}, все подходы к решению задачи классификации ai текстов, разделяются на два типа: ориентированные на анализ признакового пространства и ориентированные на дообучение моделей. 

1) \textbf{Анализ признакового пространства} основывается на извлечении и анализе характеристик текста - лексических, синтаксических, семантических или стилистических характеристик:~\cite{liang2023gpt},~\cite{yu2023gpt},~\cite{yang2023dna}.


2) \textbf{Дообучение больших языковых моделей} основывается на изучении параметров и возможностей модели и последующем дообучении модели на данных к задаче классификации текстов:~\cite{wolf2019huggingface},~\cite{wolf2019huggingface},~\cite{qasim2022fine}.


\paragraph{Цели}
Исследование снижения пространства обучаемых параметров при помощи разложения матриц.

\paragraph{Методы}
Низкоранговое разложение(англ. Low Rank Adaptation) матриц параметров в больших языковых моделях.

\paragraph{Теоретическая значимость}
В работе проведен теоретический анализ проблемы снижения размерности пространства обучаемых параметров. Доказана теорема об применимости модели BERT~\cite{vaswani2017attention} с адаптером LoRA к задаче многоклассовой классификации. 

\paragraph{Практическая значимость}
Проведен вычислительный эксперимент, показывающий улучшение качества и экономию ресурсов при решении задачи классификации текстов.